{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qpqQnuvad4jK"
   },
   "source": [
    "# ***Natural Language Processing (NLP)***\n",
    "Everything we express (either verbally or in written) carries huge amounts of information. The topic we choose, our tone, our selection of words, everything adds some type of information that can be interpreted and value extracted from it. In theory, we can understand and even predict human behaviour using that information.\n",
    "\n",
    "Data generated from conversations, declarations or even tweets are examples of unstructured data. Unstructured data doesn’t fit neatly into the traditional row and column structure of relational databases, and represent the vast majority of data available in the actual world. It is messy and hard to manipulate.\n",
    "\n",
    "#** Natural Language Processing or NLP is a field of Artificial Intelligence that gives the machines the ability to read, understand and derive meaning from human languages.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xjkLXRcwfA5a"
   },
   "source": [
    "# **Basic Python For NLP.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "id": "u_f7Vdb-F6oX",
    "outputId": "e77068d5-9897-4da1-8262-9d6632183eda"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hii my juniur , How are you\n"
     ]
    }
   ],
   "source": [
    "print('Hii my juniur , How are you')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "id": "UYNl_FDSGVhH",
    "outputId": "70a4893f-d586-43ed-802e-26deb31bf5c9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "My name is  Harsh Mishra\n"
     ]
    }
   ],
   "source": [
    "var='Harsh Mishra'\n",
    "print('My name is ',var)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "id": "ReCegOUnGoE8",
    "outputId": "fba51920-2f82-4059-d849-548c09aa68e1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "My name is Harsh Mishra \n"
     ]
    }
   ],
   "source": [
    "var='Harsh Mishra'\n",
    "print('My name is {} '.format(var))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "id": "PFXXsLvMGyMT",
    "outputId": "15af611c-1460-44a4-b200-8c9b155b8fa1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " My friend name is Shantanu Pandey\n"
     ]
    }
   ],
   "source": [
    "var='Shantanu Pandey'\n",
    "print(' My friend name is {}'.format(var))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "id": "DXeMp2mOG_dN",
    "outputId": "8489405b-f16f-40fa-90ee-590c1e36a83d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "My friend name is Tariq\n"
     ]
    }
   ],
   "source": [
    "var='Tariq'\n",
    "print(f'My friend name is {var}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "id": "HyYktsE7HN7p",
    "outputId": "bf85f220-d593-41f9-b406-1b7aa960f7e5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "value of spam harsh\n"
     ]
    }
   ],
   "source": [
    "spam=['harsh','tariq','shantanu','ayub']\n",
    "print(f\"value of spam {spam[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "id": "9MPDrHuVHkp4",
    "outputId": "2b2cb7a1-07c3-42a8-f4d9-0ebc89f180e1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "value of spam shantanu\n"
     ]
    }
   ],
   "source": [
    "spam=['harsh','tariq','shantanu','ayub']\n",
    "print(f\"value of spam {spam[2]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HGwAtCntHo_C"
   },
   "outputs": [],
   "source": [
    "dict={'name':'harsh','age':20}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "id": "3wfqaelCH18B",
    "outputId": "755c8aa6-1112-4fbb-f9fc-a53eac69f9c8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vale of dicts harsh\n"
     ]
    }
   ],
   "source": [
    "print(f\"vale of dicts {dict['name']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "g31IVBZ8ICyK"
   },
   "outputs": [],
   "source": [
    "#RE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uDt-7h6vJRPP"
   },
   "outputs": [],
   "source": [
    "txt='My phone number is 1265-125-1024'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "id": "12G0imoHJcPZ",
    "outputId": "ebf90071-d275-488f-81b6-6737a3a9659f"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'My phone number is 1265-125-1024'"
      ]
     },
     "execution_count": 12,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "id": "Ysd4ftPPJc6C",
    "outputId": "5a437936-2aed-4433-d5b4-25a32b9313e3"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 13,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'phone' in txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "id": "Rebka36tJfGi",
    "outputId": "e709fa4f-9230-47fd-e94a-8da7e69d9108"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 14,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'1265-125-1024' in txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_s0WqYeyfVtT"
   },
   "source": [
    "# ***RegEx Module***\n",
    "Python has a built-in package called re, which can be used to work with Regular Expressions.\n",
    "\n",
    "# ***REegEx Function***\n",
    "\n",
    "\n",
    "\n",
    "***1.   findall:***\tReturns a list containing all matches\n",
    "\n",
    "***2.   search:***\tReturns a Match object if there is a match anywhere in the string\n",
    "\n",
    "\n",
    "***3.   split:***\tReturns a list where the string has been split at each match\n",
    "\n",
    "***4.   sub:***\tReplaces one or many matches with a string\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2qZcW8DUfOWX"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lH4NZicOJozL"
   },
   "outputs": [],
   "source": [
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xZEk0RVtJsJY"
   },
   "outputs": [],
   "source": [
    "pattern='phone'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "id": "WLJGu9xpJvqG",
    "outputId": "2d4bc040-81b7-497f-82a4-9528291a88cb"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<_sre.SRE_Match object; span=(3, 8), match='phone'>"
      ]
     },
     "execution_count": 17,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "re.search(pattern,txt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "id": "I4dyJPzfJ2K8",
    "outputId": "3b5444da-fff3-4156-b7bd-2054fdef6487"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3, 8)"
      ]
     },
     "execution_count": 18,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "var=re.search(pattern,txt)\n",
    "var.span()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "id": "jSh91PB7KJJW",
    "outputId": "8fd1b2db-717d-4183-a1a5-1fcf7d849ec8"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 19,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "var.start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "id": "f_L9URUTKZbJ",
    "outputId": "e098ec2c-65e5-48f3-c904-b08842b619db"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8"
      ]
     },
     "execution_count": 20,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "var.end()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "iYneyjKXKbpt"
   },
   "outputs": [],
   "source": [
    "txt='harsh is good boy and harsh is bad also'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "id": "jiX7BUG8Krgs",
    "outputId": "9503404f-edae-4b4e-afa7-aa3146f31547"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<_sre.SRE_Match object; span=(0, 5), match='harsh'>"
      ]
     },
     "execution_count": 22,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pattern='harsh'\n",
    "re.search(pattern,txt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "h52MAAnZK0sO"
   },
   "outputs": [],
   "source": [
    "var=re.search(pattern,txt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "id": "5gY63KQ1K9Y5",
    "outputId": "ad496bec-22b5-43ba-8f2d-668d9ec7f314"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0, 5)"
      ]
     },
     "execution_count": 24,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "var.span()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "id": "diZ9UHwvK-jb",
    "outputId": "607ad3dc-f1b2-4cd2-ea01-c86641ac2055"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 25,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "var.start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EkNFcz57LAWG"
   },
   "outputs": [],
   "source": [
    "all=re.findall(pattern,txt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "id": "BnUjwSnCLKHm",
    "outputId": "31fa9c7c-9b01-4d25-bfbb-d9590d424410"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 27,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 52
    },
    "id": "7-sbkbV4LPCD",
    "outputId": "fc90136a-3778-46a5-e43a-2a7bb10aabae"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "harsh\n",
      "harsh\n"
     ]
    }
   ],
   "source": [
    "for match in all:\n",
    "  print(match)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qBrPDMTPLag0"
   },
   "outputs": [],
   "source": [
    "text='My phone number is 111-2222-111'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EiDv2LrpLs1N"
   },
   "outputs": [],
   "source": [
    "pattern=r'\\d\\d\\d-\\d\\d\\d\\d-\\d\\d\\d'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qEzm4l3TL4bU"
   },
   "outputs": [],
   "source": [
    "phone=re.search(pattern,text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "id": "ot7dZcuUL-Ej",
    "outputId": "0ac31565-5805-4e4f-85e3-d7758b3c60f9"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<_sre.SRE_Match object; span=(19, 31), match='111-2222-111'>"
      ]
     },
     "execution_count": 32,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "phone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "upSvDSgGMQJ6"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "id": "_v5sQw9NMCSx",
    "outputId": "3c163223-51d4-4116-bcfe-586855bc6a1a"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'111-2222-111'"
      ]
     },
     "execution_count": 33,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "phone.group()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "h0cn7cxAMGM3"
   },
   "outputs": [],
   "source": [
    "pattern=r'\\d{3}-\\d{4}-\\d{3}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GRFBM3HqMKpo"
   },
   "outputs": [],
   "source": [
    "phone=re.search(pattern,text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "--TN7pMEMMhF"
   },
   "outputs": [],
   "source": [
    "phone=re.search(pattern,text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "id": "Zxz_etmPMRT8",
    "outputId": "e2ad7dde-c567-456e-9408-42f7de15d3d4"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<_sre.SRE_Match object; span=(19, 31), match='111-2222-111'>"
      ]
     },
     "execution_count": 37,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "phone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "id": "_nPsnTjkMUUk",
    "outputId": "04d64c38-5f55-4370-c9b8-b90174031d89"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'111-2222-111'"
      ]
     },
     "execution_count": 38,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "phone.group()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "id": "J4Bq0KnfMcx-",
    "outputId": "277c1780-4c41-4b64-e27c-80410284ce5e"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'111-2222-111'"
      ]
     },
     "execution_count": 39,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "phone.group()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7YIkxKjmM1lG"
   },
   "outputs": [],
   "source": [
    "pattern=r'(\\d{3})-(\\d{4})-(\\d{3})'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vCUA-UXZNL-Q"
   },
   "outputs": [],
   "source": [
    "phone=re.search(pattern,text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "id": "Xr26eXK9NOZ4",
    "outputId": "977195dd-ab0c-43e6-ff70-3daefafdc44e"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2222'"
      ]
     },
     "execution_count": 42,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "phone.group(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "id": "V2wFi5NENRnf",
    "outputId": "dc83a6e3-6556-479d-f246-bbbc62ca180d"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<_sre.SRE_Match object; span=(14, 19), match='woman'>"
      ]
     },
     "execution_count": 43,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "re.search(r\"w.man\",\"this man  and woman is my friend\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "id": "E0TBnEzWNwkM",
    "outputId": "fb65614e-70cb-488b-dc6f-db884ba49b5d"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<_sre.SRE_Match object; span=(5, 8), match='man'>"
      ]
     },
     "execution_count": 44,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "re.search(r\"man\",\"this man and woman is my friend\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "id": "ZhhaPbcQNzpJ",
    "outputId": "ef05caeb-3224-48e2-b336-0e670d543818"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['man', 'man']"
      ]
     },
     "execution_count": 45,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "re.findall(r\"man\",\"this man and woman is my friend\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "id": "6VXRNfZdOI1J",
    "outputId": "ebfc2acf-1527-4d19-85c8-4d64e5fdc2b4"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['3']"
      ]
     },
     "execution_count": 46,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "re.findall(r\"\\d$\",\"This is 3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "id": "Z-7OO1tVOZKk",
    "outputId": "dc45594e-f9f6-4d53-f7c2-71c7b8260336"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['88']"
      ]
     },
     "execution_count": 47,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "re.findall(r\"^\\d\\d\",\"88 This is 3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_KJASmrlOkD0"
   },
   "outputs": [],
   "source": [
    "txt=' 1 is my lucky number 25 is unluky 44'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 585
    },
    "id": "70h_6YU8PL1T",
    "outputId": "92e434a9-93d3-4217-c0d6-f376e81d4e7d"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[' ',\n",
       " ' ',\n",
       " 'i',\n",
       " 's',\n",
       " ' ',\n",
       " 'm',\n",
       " 'y',\n",
       " ' ',\n",
       " 'l',\n",
       " 'u',\n",
       " 'c',\n",
       " 'k',\n",
       " 'y',\n",
       " ' ',\n",
       " 'n',\n",
       " 'u',\n",
       " 'm',\n",
       " 'b',\n",
       " 'e',\n",
       " 'r',\n",
       " ' ',\n",
       " ' ',\n",
       " 'i',\n",
       " 's',\n",
       " ' ',\n",
       " 'u',\n",
       " 'n',\n",
       " 'l',\n",
       " 'u',\n",
       " 'k',\n",
       " 'y',\n",
       " ' ']"
      ]
     },
     "execution_count": 49,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "re.findall(r\"[^\\d]\",txt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "id": "KHU0w4nDPaOc",
    "outputId": "216b04f0-1aad-432d-cc7a-0796ee53af3b"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[' ', ' is my lucky number ', ' is unluky ']"
      ]
     },
     "execution_count": 50,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "re.findall(r\"[^\\d]+\",txt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "e05AfoUnPj4u"
   },
   "outputs": [],
   "source": [
    "text=\"wow! this is amazing.am i like it?re\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "id": "pvEmmA2iPpEY",
    "outputId": "2870d7a2-a295-4ab7-fac8-75c2422d546a"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['wow', ' this is amazing', 'am i like it', 're']"
      ]
     },
     "execution_count": 52,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "re.findall(r\"[^!.?]+\",text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CGE9MmG4Qcil"
   },
   "outputs": [],
   "source": [
    "var=re.findall(r\"[^!.?]+\",text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "id": "GTHodkdSQ2XT",
    "outputId": "18c01c28-18c8-4879-e51d-1dc7c26b175f"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'wow this is amazingam i like itre'"
      ]
     },
     "execution_count": 54,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "''.join(var)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CaUQyYydQ5tN"
   },
   "outputs": [],
   "source": [
    "import nltk "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vPtCpMafR0DP"
   },
   "outputs": [],
   "source": [
    "from nltk.corpus import gutenberg as gt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 70
    },
    "id": "oB2sclYkR5eU",
    "outputId": "ecaa4dcb-1dc3-4d08-f1b5-896d61067e6d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package gutenberg to /root/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/gutenberg.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 57,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    " nltk.download('gutenberg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 336
    },
    "id": "QSFVS1ESSCsa",
    "outputId": "3a8a777c-64b7-4ea8-c1b1-f6c2649752a8"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['austen-emma.txt',\n",
       " 'austen-persuasion.txt',\n",
       " 'austen-sense.txt',\n",
       " 'bible-kjv.txt',\n",
       " 'blake-poems.txt',\n",
       " 'bryant-stories.txt',\n",
       " 'burgess-busterbrown.txt',\n",
       " 'carroll-alice.txt',\n",
       " 'chesterton-ball.txt',\n",
       " 'chesterton-brown.txt',\n",
       " 'chesterton-thursday.txt',\n",
       " 'edgeworth-parents.txt',\n",
       " 'melville-moby_dick.txt',\n",
       " 'milton-paradise.txt',\n",
       " 'shakespeare-caesar.txt',\n",
       " 'shakespeare-hamlet.txt',\n",
       " 'shakespeare-macbeth.txt',\n",
       " 'whitman-leaves.txt']"
      ]
     },
     "execution_count": 58,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gt.fileids()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 52
    },
    "id": "vXz8zh5kSN-t",
    "outputId": "982ab4e4-7b7e-4633-aeeb-d3f1a05d2919"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['[', 'The', 'Tragedie', 'of', 'Hamlet', 'by', ...]\n",
      "37360\n"
     ]
    }
   ],
   "source": [
    "#words func\n",
    "shkspr_hmlt=gt.words('shakespeare-hamlet.txt')\n",
    "print(shkspr_hmlt)\n",
    "print(len(shkspr_hmlt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 70
    },
    "id": "gz7eVvrKTByo",
    "outputId": "fcbc79c4-b4c9-4404-c18c-e740f910ef6f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 60,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "shkspr_hmlt=gt.raw('shakespeare-hamlet.txt')\n",
    "#print(shkspr_hmlt)\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "XvsPJ1r0ThpB",
    "outputId": "120b8a93-241c-4160-bb6e-340749a553fd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data for file ID: austen-emma.txt\n",
      "Number Of Words: 192427 Number Of sentances: 7752\n",
      "Words: ['[', 'Emma', 'by', 'Jane', 'Austen', '1816', ']', ...]\n",
      "\n",
      "\n",
      " Data for file ID: austen-persuasion.txt\n",
      "Number Of Words: 98171 Number Of sentances: 3747\n",
      "Words: ['[', 'Persuasion', 'by', 'Jane', 'Austen', '1818', ...]\n",
      "\n",
      "\n",
      " Data for file ID: austen-sense.txt\n",
      "Number Of Words: 141576 Number Of sentances: 4999\n",
      "Words: ['[', 'Sense', 'and', 'Sensibility', 'by', 'Jane', ...]\n",
      "\n",
      "\n",
      " Data for file ID: bible-kjv.txt\n",
      "Number Of Words: 1010654 Number Of sentances: 30103\n",
      "Words: ['[', 'The', 'King', 'James', 'Bible', ']', 'The', ...]\n",
      "\n",
      "\n",
      " Data for file ID: blake-poems.txt\n",
      "Number Of Words: 8354 Number Of sentances: 438\n",
      "Words: ['[', 'Poems', 'by', 'William', 'Blake', '1789', ']', ...]\n",
      "\n",
      "\n",
      " Data for file ID: bryant-stories.txt\n",
      "Number Of Words: 55563 Number Of sentances: 2863\n",
      "Words: ['[', 'Stories', 'to', 'Tell', 'to', 'Children', 'by', ...]\n",
      "\n",
      "\n",
      " Data for file ID: burgess-busterbrown.txt\n",
      "Number Of Words: 18963 Number Of sentances: 1054\n",
      "Words: ['[', 'The', 'Adventures', 'of', 'Buster', 'Bear', ...]\n",
      "\n",
      "\n",
      " Data for file ID: carroll-alice.txt\n",
      "Number Of Words: 34110 Number Of sentances: 1703\n",
      "Words: ['[', 'Alice', \"'\", 's', 'Adventures', 'in', ...]\n",
      "\n",
      "\n",
      " Data for file ID: chesterton-ball.txt\n",
      "Number Of Words: 96996 Number Of sentances: 4779\n",
      "Words: ['[', 'The', 'Ball', 'and', 'The', 'Cross', 'by', 'G', ...]\n",
      "\n",
      "\n",
      " Data for file ID: chesterton-brown.txt\n",
      "Number Of Words: 86063 Number Of sentances: 3806\n",
      "Words: ['[', 'The', 'Wisdom', 'of', 'Father', 'Brown', 'by', ...]\n",
      "\n",
      "\n",
      " Data for file ID: chesterton-thursday.txt\n",
      "Number Of Words: 69213 Number Of sentances: 3742\n",
      "Words: ['[', 'The', 'Man', 'Who', 'Was', 'Thursday', 'by', ...]\n",
      "\n",
      "\n",
      " Data for file ID: edgeworth-parents.txt\n",
      "Number Of Words: 210663 Number Of sentances: 10230\n",
      "Words: ['[', 'The', 'Parent', \"'\", 's', 'Assistant', ',', ...]\n",
      "\n",
      "\n",
      " Data for file ID: melville-moby_dick.txt\n",
      "Number Of Words: 260819 Number Of sentances: 10059\n",
      "Words: ['[', 'Moby', 'Dick', 'by', 'Herman', 'Melville', ...]\n",
      "\n",
      "\n",
      " Data for file ID: milton-paradise.txt\n",
      "Number Of Words: 96825 Number Of sentances: 1851\n",
      "Words: ['[', 'Paradise', 'Lost', 'by', 'John', 'Milton', ...]\n",
      "\n",
      "\n",
      " Data for file ID: shakespeare-caesar.txt\n",
      "Number Of Words: 25833 Number Of sentances: 2163\n",
      "Words: ['[', 'The', 'Tragedie', 'of', 'Julius', 'Caesar', ...]\n",
      "\n",
      "\n",
      " Data for file ID: shakespeare-hamlet.txt\n",
      "Number Of Words: 37360 Number Of sentances: 3106\n",
      "Words: ['[', 'The', 'Tragedie', 'of', 'Hamlet', 'by', ...]\n",
      "\n",
      "\n",
      " Data for file ID: shakespeare-macbeth.txt\n",
      "Number Of Words: 23140 Number Of sentances: 1907\n",
      "Words: ['[', 'The', 'Tragedie', 'of', 'Macbeth', 'by', ...]\n",
      "\n",
      "\n",
      " Data for file ID: whitman-leaves.txt\n",
      "Number Of Words: 154883 Number Of sentances: 4250\n",
      "Words: ['[', 'Leaves', 'of', 'Grass', 'by', 'Walt', 'Whitman', ...]\n",
      "\n",
      "\n",
      " "
     ]
    }
   ],
   "source": [
    "for fileid in gt.fileids():\n",
    "    num_words=len(gt.words(fileid))\n",
    "    num_sents=len(gt.sents(fileid))\n",
    "    print('Data for file ID:',fileid)\n",
    "    print('Number Of Words:',num_words,'Number Of sentances:',num_sents)\n",
    "    print('Words:',gt.words(fileid),end='\\n\\n\\n ')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zPW7zqaehH39"
   },
   "source": [
    "# ***Tokenization:***\n",
    "Is the process of segmenting running text into sentences and words. In essence, it’s the task of cutting a text into pieces called tokens, and at the same time throwing away certain characters, such as punctuation.\n",
    "\n",
    "Tokenization can remove punctuation too, easing the path to a proper word segmentation but also triggering possible complications. In the case of periods that follow abbreviation (e.g. dr.), the period following that abbreviation should be considered as part of the same token and not be removed.\n",
    "\n",
    "The tokenization process can be particularly problematic when dealing with biomedical text domains which contain lots of hyphens, parentheses, and other punctuation marks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Zl1YaHENUEoj"
   },
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize,sent_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "id": "sBQrOp1rUtlB",
    "outputId": "c76cd971-13bb-4652-add0-e5104b1552d4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['DO', 'something', 'with', 'it']\n"
     ]
    }
   ],
   "source": [
    "mytext=\"DO something with it\"\n",
    "print(word_tokenize(mytext))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "id": "Oqn4NRPhUw1a",
    "outputId": "aa68f5b2-4790-43cc-d192-61fd34dff6e1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['My', 'name', 'is', 'Harsh', 'i', 'wo', \"n't\", 'from', 'etawah']\n"
     ]
    }
   ],
   "source": [
    "mytext=\"My name is Harsh i won't from etawah\"\n",
    "print(word_tokenize(mytext))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qmjQZDsnU47Q"
   },
   "outputs": [],
   "source": [
    "from nltk.tokenize import WordPunctTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "id": "pnTDwFexU-Dn",
    "outputId": "f6f2dd6c-b631-445b-a169-39700d25d198"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['My', 'name', 'is', 'Harsh', 'i', 'won', \"'\", 't', 'from', 'etawah']"
      ]
     },
     "execution_count": 66,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mytext=\"My name is Harsh i won't from etawah\"\n",
    "tokenizer=WordPunctTokenizer()\n",
    "tokenizer.tokenize(mytext)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 70
    },
    "id": "zpW5zjKwVDkg",
    "outputId": "edaec14d-343a-4ce4-cef5-dea380c7f739"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Hey, i am harsh.',\n",
       " 'Wanna do something defferenrt.',\n",
       " \"Don't you think that not so easy\"]"
      ]
     },
     "execution_count": 67,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "para=\"Hey, i am harsh. Wanna do something defferenrt. Don't you think that\\\n",
    " not so easy\"\n",
    "sent_tokenize(para)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "id": "mR_kmeFhVLBG",
    "outputId": "66dea169-befe-43ae-f434-bb4557f748fa"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Jim', 'is', 'bringing', 'his', 'bulldog', 'to', 'eat', 'at', 'friendly', '?']\n"
     ]
    }
   ],
   "source": [
    "sentence=\"Jim is bringing his bulldog to eat at friendly?\"\n",
    "tokens=word_tokenize(sentence)\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "K3DQjMIKh5x0"
   },
   "source": [
    "# ***Stop Words Removal:***\n",
    "Includes getting rid of common language articles, pronouns and prepositions such as “and”, “the” or “to” in English. In this process some very common words that appear to provide little or no value to the NLP objective are filtered and excluded from the text to be processed, hence removing widespread and frequent terms that are not informative about the corresponding text.\n",
    "\n",
    "Stop words can be safely ignored by carrying out a lookup in a pre-defined list of keywords, freeing up database space and improving processing time.\n",
    "\n",
    "The thing is stop words removal can wipe out relevant information and modify the context in a given sentence. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "O7EYPDtiVPxo",
    "outputId": "22a135d2-13be-4464-c6b1-c4b091c7138a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'a',\n",
       " 'about',\n",
       " 'above',\n",
       " 'after',\n",
       " 'again',\n",
       " 'against',\n",
       " 'ain',\n",
       " 'all',\n",
       " 'am',\n",
       " 'an',\n",
       " 'and',\n",
       " 'any',\n",
       " 'are',\n",
       " 'aren',\n",
       " \"aren't\",\n",
       " 'as',\n",
       " 'at',\n",
       " 'be',\n",
       " 'because',\n",
       " 'been',\n",
       " 'before',\n",
       " 'being',\n",
       " 'below',\n",
       " 'between',\n",
       " 'both',\n",
       " 'but',\n",
       " 'by',\n",
       " 'can',\n",
       " 'couldn',\n",
       " \"couldn't\",\n",
       " 'd',\n",
       " 'did',\n",
       " 'didn',\n",
       " \"didn't\",\n",
       " 'do',\n",
       " 'does',\n",
       " 'doesn',\n",
       " \"doesn't\",\n",
       " 'doing',\n",
       " 'don',\n",
       " \"don't\",\n",
       " 'down',\n",
       " 'during',\n",
       " 'each',\n",
       " 'few',\n",
       " 'for',\n",
       " 'from',\n",
       " 'further',\n",
       " 'had',\n",
       " 'hadn',\n",
       " \"hadn't\",\n",
       " 'has',\n",
       " 'hasn',\n",
       " \"hasn't\",\n",
       " 'have',\n",
       " 'haven',\n",
       " \"haven't\",\n",
       " 'having',\n",
       " 'he',\n",
       " 'her',\n",
       " 'here',\n",
       " 'hers',\n",
       " 'herself',\n",
       " 'him',\n",
       " 'himself',\n",
       " 'his',\n",
       " 'how',\n",
       " 'i',\n",
       " 'if',\n",
       " 'in',\n",
       " 'into',\n",
       " 'is',\n",
       " 'isn',\n",
       " \"isn't\",\n",
       " 'it',\n",
       " \"it's\",\n",
       " 'its',\n",
       " 'itself',\n",
       " 'just',\n",
       " 'll',\n",
       " 'm',\n",
       " 'ma',\n",
       " 'me',\n",
       " 'mightn',\n",
       " \"mightn't\",\n",
       " 'more',\n",
       " 'most',\n",
       " 'mustn',\n",
       " \"mustn't\",\n",
       " 'my',\n",
       " 'myself',\n",
       " 'needn',\n",
       " \"needn't\",\n",
       " 'no',\n",
       " 'nor',\n",
       " 'not',\n",
       " 'now',\n",
       " 'o',\n",
       " 'of',\n",
       " 'off',\n",
       " 'on',\n",
       " 'once',\n",
       " 'only',\n",
       " 'or',\n",
       " 'other',\n",
       " 'our',\n",
       " 'ours',\n",
       " 'ourselves',\n",
       " 'out',\n",
       " 'over',\n",
       " 'own',\n",
       " 're',\n",
       " 's',\n",
       " 'same',\n",
       " 'shan',\n",
       " \"shan't\",\n",
       " 'she',\n",
       " \"she's\",\n",
       " 'should',\n",
       " \"should've\",\n",
       " 'shouldn',\n",
       " \"shouldn't\",\n",
       " 'so',\n",
       " 'some',\n",
       " 'such',\n",
       " 't',\n",
       " 'than',\n",
       " 'that',\n",
       " \"that'll\",\n",
       " 'the',\n",
       " 'their',\n",
       " 'theirs',\n",
       " 'them',\n",
       " 'themselves',\n",
       " 'then',\n",
       " 'there',\n",
       " 'these',\n",
       " 'they',\n",
       " 'this',\n",
       " 'those',\n",
       " 'through',\n",
       " 'to',\n",
       " 'too',\n",
       " 'under',\n",
       " 'until',\n",
       " 'up',\n",
       " 've',\n",
       " 'very',\n",
       " 'was',\n",
       " 'wasn',\n",
       " \"wasn't\",\n",
       " 'we',\n",
       " 'were',\n",
       " 'weren',\n",
       " \"weren't\",\n",
       " 'what',\n",
       " 'when',\n",
       " 'where',\n",
       " 'which',\n",
       " 'while',\n",
       " 'who',\n",
       " 'whom',\n",
       " 'why',\n",
       " 'will',\n",
       " 'with',\n",
       " 'won',\n",
       " \"won't\",\n",
       " 'wouldn',\n",
       " \"wouldn't\",\n",
       " 'y',\n",
       " 'you',\n",
       " \"you'd\",\n",
       " \"you'll\",\n",
       " \"you're\",\n",
       " \"you've\",\n",
       " 'your',\n",
       " 'yours',\n",
       " 'yourself',\n",
       " 'yourselves'}"
      ]
     },
     "execution_count": 69,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "stop_words=set(stopwords.words('english'))\n",
    "stop_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "id": "csmEUkeaVYP5",
    "outputId": "bdb210a6-c36d-4490-c2fc-f63d869d5496"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Jim', 'bringing', 'bulldog', 'eat', 'friendly', '?']\n"
     ]
    }
   ],
   "source": [
    "tokens=[w for w in tokens if not w in stop_words]\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JIl0Hc5KV2F-"
   },
   "outputs": [],
   "source": [
    "txt=\"My email is hmisarsh8192@gmail.com and my phone number is 9557566140\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Xmq9yHHbXCws"
   },
   "outputs": [],
   "source": [
    "my=word_tokenize(txt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 265
    },
    "id": "ZJN5PlNE2wsD",
    "outputId": "ceb7ff11-f3ef-4370-f28f-33b99669aa81"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['My',\n",
       " 'email',\n",
       " 'is',\n",
       " 'hmisarsh8192',\n",
       " '@',\n",
       " 'gmail',\n",
       " '.',\n",
       " 'com',\n",
       " 'and',\n",
       " 'my',\n",
       " 'phone',\n",
       " 'number',\n",
       " 'is',\n",
       " '9557566140']"
      ]
     },
     "execution_count": 73,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.tokenize(txt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 230
    },
    "id": "trWPK3cbXJlu",
    "outputId": "9bc7c15f-60a3-44d5-e7d6-96d26b7d8746"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['My',\n",
       " 'email',\n",
       " 'is',\n",
       " 'hmisarsh8192',\n",
       " '@',\n",
       " 'gmail.com',\n",
       " 'and',\n",
       " 'my',\n",
       " 'phone',\n",
       " 'number',\n",
       " 'is',\n",
       " '9557566140']"
      ]
     },
     "execution_count": 74,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NMAG6hqSfCFC"
   },
   "source": [
    "# **Stemming:**\n",
    "Refers to the process of slicing the end or the beginning of words with the intention of removing affixes (lexical additions to the root of the word).\n",
    "\n",
    "#** Affixes that are attached at the beginning of the word are called prefixes (e.g. “astro” in the word “astrobiology”) and the ones attached at the end of the word are called suffixes (e.g. “ful” in the word “helpful”**).\n",
    "\n",
    "\n",
    "The problem is that affixes can create or expand new forms of the same word (called inflectional affixes), or even create new words themselves (called derivational affixes). In English, prefixes are always derivational (the affix creates a new word as in the example of the prefix “eco” in the word “ecosystem”), but suffixes can be derivational (the affix creates a new word as in the example of the suffix “ist” in the word “guitarist”) or inflectional (the affix creates a new form of word as in the example of the suffix “er” in the word “faster”).\n",
    "\n",
    "\n",
    "Playing------ Play\n",
    "\n",
    "News------- New (Wrong)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RAWtkMWSXK9P"
   },
   "outputs": [],
   "source": [
    "from nltk.stem import PorterStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2vRStH8lXs6d"
   },
   "outputs": [],
   "source": [
    "mm=PorterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "h6euLqkhbmT_"
   },
   "outputs": [],
   "source": [
    "word=[\"learning\",'running','Learning','Tuning','runner','easyily','fairly']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 141
    },
    "id": "B1VXas_wb7eI",
    "outputId": "6050e6d0-1743-46d9-88aa-d4256fb37b69"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "learning-----learn\n",
      "running-----run\n",
      "Learning-----learn\n",
      "Tuning-----tune\n",
      "runner-----runner\n",
      "easyily-----easyili\n",
      "fairly-----fairli\n"
     ]
    }
   ],
   "source": [
    "for i in word:\n",
    "  print(i+'-----'+mm.stem(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hMhyogu9chs1"
   },
   "outputs": [],
   "source": [
    "from nltk.stem import SnowballStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-cgEysz1dTih"
   },
   "outputs": [],
   "source": [
    "ss=SnowballStemmer(language=\"english\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 141
    },
    "id": "Edw2gfivduF0",
    "outputId": "828b403d-da8c-4f51-d9f2-25ff903a7ac8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "learning-----learn\n",
      "running-----run\n",
      "Learning-----learn\n",
      "Tuning-----tune\n",
      "runner-----runner\n",
      "easyily-----easyili\n",
      "fairly-----fair\n"
     ]
    }
   ],
   "source": [
    "for i in word:\n",
    "  print(i+'-----'+ss.stem(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Ay8eblqhdzlu"
   },
   "outputs": [],
   "source": [
    "mylist=['generous','generation','genereously','generate']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 185
    },
    "id": "eWFSAqnZednb",
    "outputId": "a89394d5-0e80-49b6-849b-1c0e1f6f135a"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "ignored",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-82-7d51c344825b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmylist\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m   \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m'-----'\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'mylist' is not defined"
     ]
    }
   ],
   "source": [
    "for i in mylist:\n",
    "  print(i+'-----'+ss.stem(i))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LuRkmoOykpTw"
   },
   "source": [
    "# ***Part_Of_speech Tagging:***\n",
    "\n",
    "Basically, the goal of a POS tagger is to assign linguistic (mostly grammatical) information to sub-sentential units. Such units are called tokens and, most of the time, correspond to words and symbols (e.g. punctuation).\n",
    "\n",
    "\n",
    "CC coordinating conjunction\n",
    "\n",
    "CD cardinal digit\n",
    "\n",
    "DT determiner\n",
    "\n",
    "EX existential there (like: “there is” … think of it like “there exists”)\n",
    "\n",
    "FW foreign word\n",
    "\n",
    "IN preposition/subordinating conjunction\n",
    "\n",
    "JJ adjective ‘big’\n",
    "\n",
    "JJR adjective, comparative ‘bigger’\n",
    "\n",
    "JJS adjective, superlative ‘biggest’\n",
    "\n",
    "LS list marker 1)\n",
    "\n",
    "MD modal could, will\n",
    "\n",
    "NN noun, singular ‘desk’\n",
    "\n",
    "NNS noun plural ‘desks’\n",
    "\n",
    "NNP proper noun, singular ‘Harrison’\n",
    "\n",
    "NNPS proper noun, plural ‘Americans’\n",
    "\n",
    "PDT predeterminer ‘all the kids’\n",
    "\n",
    "POS possessive ending parent‘s\n",
    "\n",
    "PRP personal pronoun I, he, she"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 105
    },
    "id": "bSlR2I8oeiE6",
    "outputId": "27aeb465-a2a9-46e4-895c-98e8338cff10"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /root/nltk_data...\n",
      "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n",
      "[('Jim', 'NNP'), ('bringing', 'VBG'), ('bulldog', 'JJ'), ('eat', 'NN'), ('friendly', 'RB'), ('?', '.')]\n",
      "[('Jim', 'NNP'), ('bringing', 'VBG'), ('bulldog', 'JJ'), ('eat', 'NN'), ('friendly', 'RB'), ('?', '.')] Nouns [('eat', 'NN')]\n"
     ]
    }
   ],
   "source": [
    "nltk.download('averaged_perceptron_tagger')\n",
    "tags=nltk.pos_tag(tokens)\n",
    "#pos (part of speech)\n",
    "print(tags)\n",
    "print(tags,'Nouns',[t for t in tags if t[1]=='NN'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 52
    },
    "id": "OJbhhmFvlCej",
    "outputId": "e7e6cb74-5f6b-4925-c7b9-e3cc05cb9831"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Jim', 'NNP'), ('bringing', 'VBG'), ('bulldog', 'JJ'), ('eat', 'NN'), ('friendly', 'RB'), ('?', '.')]\n",
      "Nouns [('eat', 'NN')]\n"
     ]
    }
   ],
   "source": [
    "tags=nltk.pos_tag(tokens)\n",
    "#pos (part of speech)\n",
    "print(tags)\n",
    "print('Nouns',[t for t in tags if t[1]=='NN'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zPaAewTllSo1"
   },
   "outputs": [],
   "source": [
    "import nltk \n",
    "import re\n",
    "from nltk.corpus import gutenberg as gt\n",
    "\n",
    "shkspr_hmlt=gt.words('shakespeare-hamlet.txt')\n",
    "words=gt.words('shakespeare-hamlet.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 90
    },
    "id": "0q5tEDbWlX9q",
    "outputId": "67eaada9-81df-4c53-cdb3-8ffbb19da30d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "320\n",
      "{'surprized', 'swallowed', 'seated', 'combined', 'wed', 'perceiued', 'sacred', 'allowed', 'departed', 'parted', 'winnowed', 'combatted', 'beautifed', 'vnbaited', 'reuerted', 'incensed', 'cried', 'moued', 'stalled', 'buried', 'vnuallued', 'vnmastred', 'frighted', 'vnnerued', 'releeued', 'red', 'Seed', 'rowsed', 'intreated', 'cursed', 'ratled', 'fatted', 'neglected', 'defeated', 'payed', 'answered', 'ratified', 'teamed', 'considered', 'tumbled', 'vnweeded', 'limed', 'concluded', 'shipped', 'died', 'deed', 'naked', 'vnimproued', 'fretted', 'suffred', 'sledded', 'encountred', 'Damned', 'vnfortified', 'Diuided', 'sanctified', 'Vnsifted', 'imitated', 'pittied', 'dilated', 'speed', 'behaued', 'changed', 'vttered', 'wrinkled', 'indued', 'vnsanctified', 'Addicted', 'collected', 'coated', 'vnshaped', 'conuerted', 'timbred', 'greeted', 'couched', 'beautified', 'Candied', 'lighted', 'affrighted', 'longed', 'breed', 'gilded', 'escorted', 'fitted', 'ended', 'veyled', 'giued', 'forged', 'Vnhouzzled', 'beleeued', 'dyed', 'sized', 'grained', 'greeued', 'muddied', 'boorded', 'maimed', 'demanded', 'chaunted', 'started', 'remembred', 'noted', 'wretched', 'arriued', 'wounded', 'deiected', 'benetted', 'accounted', 'vnsmirched', 'followed', 'vnsatisfied', 'vnsinnowed', 'satisfied', 'Acted', 'conceited', 'Orbed', 'loued', 'footed', 'Blasted', 'encombred', 'Vnbated', 'remoued', 'offended', 'celebrated', 'borrowed', 'Wed', 'beautied', 'solicited', 'Bed', 'wicked', 'painted', 'corrupted', 'gaged', 'feed', 'impasted', 'compelled', 'vnlimited', 'preserued', 'Hearsed', 'stowed', 'married', 'proceeded', 'pated', 'abhorred', 'strutted', 'liued', 'bended', 'occulted', 'hoorded', 'perturbed', 'infected', 'bedded', 'leuied', 'digested', 'proceed', 'bleed', 'placed', 'gauled', 'Inobled', 'bounded', 'indeed', 'vnpolluted', 'coppied', 'Colleagued', 'commanded', 'deserued', 'hundred', 'vngalled', 'opposed', 'attended', 'tickled', 'Pursued', 'rugged', 'deceiued', 'quoted', 'lifted', 'repulsed', 'enseamed', 'distracted', 'guarded', 'bated', 'sicklied', 'setled', 'Prompted', 'Folded', 'befitted', 'Vngartred', 'fed', 'Collected', 'exceed', 'damned', 'Extorted', 'drowned', 'fortified', 'bellowed', 'iangled', 'Indeed', 'contracted', 'metled', 'vndiscouered', 'heed', 'blunted', 'wonted', 'blasted', 'inobled', 'bed', 'carried', 'need', 'laboured', 'dreaded', 'picked', 'dismantled', 'murthered', 'cryed', 'sounded', 'faded', 'Compounded', 'loosed', 'disappointed', 'spred', 'Larded', 'mingled', 'roasted'}\n",
      "['bed', 'dreaded', 'intreated', 'fortified', 'buried', 'offended', 'combatted', 'sledded', 'ratified', 'gaged', 'vnimproued', 'hoorded', 'Extorted', 'started', 'faded', 'celebrated', 'befitted', 'contracted', 'defeated', 'Colleagued', 'dilated', 'veyled', 'deiected', 'indeed', 'vnfortified', 'cried', 'dyed', 'vnweeded', 'Seed', 'fed', 'followed', 'married', 'gauled', 'married', 'wicked', 'speed', 'Indeed', 'followed', 'encountred', 'surprized', 'lifted', 'Indeed', 'indeed', 'red', 'hundred', 'vnuallued', 'deed', 'vnmastred', 'opposed', 'Vnsifted', 'sanctified', 'Indeed', 'wicked', 'Hearsed', 'remoued', 'combined', 'forged', 'wicked', 'moued', 'bed', 'cursed', 'Vnhouzzled', 'disappointed', 'Bed', 'damned', 'distracted', 'coppied', 'damned', 'Indeed', 'Indeed', 'encombred', 'perturbed', 'cursed', 'Addicted', 'heed', 'noted', 'affrighted', 'Vngartred', 'giued', 'loosed', 'bended', 'speed', 'quoted', 'commanded', 'changed', 'greeued', 'leuied', 'ended', 'beautifed', 'beautified', 'perceiued', 'repulsed', 'indeed', 'breed', 'suffred', 'wrinkled', 'Indeed', 'deserued', 'bounded', 'indeed', 'attended', 'preserued', 'indeed', 'fretted', 'coated', 'tickled', 'indeed', 'wonted', 'ratled', 'escorted', 'liued', 'hundred', 'indeed', 'vnlimited', 'loued', 'Acted', 'cried', 'digested', 'rugged', 'rugged', 'couched', 'impasted', 'damned', 'roasted', 'sized', 'vnnerued', 'painted', 'rowsed', 'inobled', 'inobled', 'Inobled', 'teamed', 'liued', 'need', 'indeed', 'metled', 'fatted', 'murthered', 'Prompted', 'distracted', 'sounded', 'behaued', 'wonted', 'beautied', 'painted', 'vndiscouered', 'sicklied', 'remembred', 'longed', 'Indeed', 'beleeued', 'loued', 'deceiued', 'married', 'wretched', 'iangled', 'Blasted', 'speed', 'neglected', 'setled', 'neglected', 'pated', 'strutted', 'bellowed', 'imitated', 'considered', 'feed', 'Candied', 'mingled', 'occulted', 'damned', 'feed', 'accounted', 'dyed', 'carried', 'Orbed', 'borrowed', 'sacred', 'wed', 'Bed', 'wed', 'collected', 'blasted', 'infected', 'frighted', 'vngalled', 'dismantled', 'breed', 'bed', 'indeed', 'footed', 'bed', 'cursed', 'stalled', 'corrupted', 'gilded', 'wicked', 'wretched', 'limed', 'bed', 'offended', 'offended', 'deed', 'deed', 'wretched', 'damned', 'deed', 'seated', 'lighted', 'indeed', 'feed', 'grained', 'enseamed', 'bed', 'blunted', 'bedded', 'liued', 'vttered', 'spred', 'bed', 'bed', 'concluded', 'deed', 'answered', 'deed', 'stowed', 'Compounded', 'demanded', 'swallowed', 'loued', 'distracted', 'releeued', 'guarded', 'indeed', 'deed', 'speed', 'red', 'indeed', 'pittied', 'vnshaped', 'Indeed', 'distracted', 'Larded', 'departed', 'Indeed', 'tumbled', 'Wed', 'bed', 'muddied', 'Diuided', 'vnsmirched', 'payed', 'fitted', 'dyed', 'bed', 'greeted', 'compelled', 'boorded', 'Pursued', 'proceeded', 'vnsinnowed', 'timbred', 'reuerted', 'naked', 'naked', 'indeed', 'cryed', 'indeed', 'indeed', 'indeed', 'vnbaited', 'Collected', 'spred', 'chaunted', 'indued', 'buried', 'drowned', 'buried', 'shipped', 'indeed', 'buried', 'picked', 'abhorred', 'died', 'buried', 'conuerted', 'maimed', 'vnsanctified', 'allowed', 'parted', 'vnpolluted', 'bed', 'cursed', 'wicked', 'deed', 'wounded', 'buried', 'Larded', 'bated', 'proceed', 'benetted', 'laboured', 'allowed', 'Folded', 'incensed', 'indeed', 'conceited', 'exceed', 'winnowed', 'satisfied', 'bleed', 'Vnbated', 'Damned', 'red', 'wretched', 'vnsatisfied', 'wounded', 'solicited', 'arriued', 'placed']\n"
     ]
    }
   ],
   "source": [
    "wrds_ndng_with_ed=[w for w in words if re.search('ed$',w)]\n",
    "print(len(wrds_ndng_with_ed))\n",
    "print((set(wrds_ndng_with_ed)))\n",
    "print(wrds_ndng_with_ed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "id": "UW_DWIFOlgmi",
    "outputId": "da79cc55-a109-4fb0-8f72-87c2c566d912"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "178\n"
     ]
    }
   ],
   "source": [
    "print(len(set([w for w in words if re.search('^a+',w)])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 52
    },
    "id": "GcoI12LxllRr",
    "outputId": "39a6ed45-482d-40a0-9081-f47402ff1072"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "learning; studying\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "text='Hey , I am mr k. Who is currenyly learning Data science and studying nlp'\n",
    "obj=nltk.Text(nltk.word_tokenize(text))\n",
    "print(obj.findall(r'<.*ing>+'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ihKVpa5lltDD"
   },
   "outputs": [],
   "source": [
    "text='Considered \"process\" while Linux will handle the process\\'s'\\\n",
    "'Life-cycle, you will need a way of intracting-with the operating system'\\\n",
    "'to manage it from a higher-lavel'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 54
    },
    "id": "AyLTut4Tl2m1",
    "outputId": "35b71401-dc21-4cb9-e607-73bf4d5c9817"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Considered', '', '\"', 'process', '\"', '', 'while', '', 'Linux', '', 'will', '', 'handle', '', 'the', '', 'process', \"'\", 'sLife', '-', 'cycle', ',', '', 'you', '', 'will', '', 'need', '', 'a', '', 'way', '', 'of', '', 'intracting', '-', 'with', '', 'the', '', 'operating', '', 'systemto', '', 'manage', '', 'it', '', 'from', '', 'a', '', 'higher', '-', 'lavel', '']\n"
     ]
    }
   ],
   "source": [
    "print(re.findall('\\w+|\\S|\\w*',text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xkxXwKGbVZXI"
   },
   "source": [
    "# ***Stemming Algorithm Examples:***\n",
    "\n",
    "Two stemming algorithms I immediately came in contact with when I first started \n",
    "\n",
    "\n",
    "using stemming were the Porter stemmer and the Snowball stemmer from NLTK. While I won’t go into a lot of details about either, I will highlight a little bit about them so that you can know even more than I did when I first started using them.\n",
    "\n",
    "\n",
    "*   ***Porter stemmer:*** This stemming algorithm is an older one. It’s from the 1980s and its main concern is removing the common endings to words so that they can be resolved to a common form. It’s not too complex and development on it is frozen. Typically, it’s a nice starting basic stemmer, but it’s not really advised to use it for any production/complex application. Instead, it has its place in research as a nice, basic stemming algorithm that can guarantee reproducibility. It also is a very gentle stemming algorithm when compared to others.\n",
    "*   ***Snowball stemmer:*** This algorithm is also known as the Porter2 stemming algorithm. It is almost universally accepted as better than the Porter stemmer, even being acknowledged as such by the individual who created the Porter stemmer. That being said, it is also more aggressive than the Porter stemmer. A lot of the things added to the Snowball stemmer were because of issues noticed with the Porter stemmer. There is about a 5% difference in the way that Snowball stems versus Porter.\n",
    "\n",
    "\n",
    "*   ***Lancaster stemmer: ***Just for fun, the Lancaster stemming algorithm is another algorithm that you can use. This one is the most aggressive stemming algorithm of the bunch. However, if you use the stemmer in NLTK, you can add your own custom rules to this algorithm very easily. It’s a good choice for that. One complaint around this stemming algorithm though is that it sometimes is overly aggressive and can really transform words into strange stems. Just make sure it does what you want it to before you go with this option!\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aOKapyk3WSCz"
   },
   "source": [
    "# ***Lemmatization:***\n",
    "We’ve talked about stemming, but what about the other side of things? How is lemmatization different? Well, if we think of stemming as just take a best guess of where to snip a word based on how it looks, lemmatization is a more calculated process. It involves resolving words to their dictionary form. In fact, a lemma of a word is its dictionary or canonical form!\n",
    "Because lemmatization is more nuanced in this respect, it requires a little more to actually make work. For lemmatization to resolve a word to its lemma, it needs to know its part of speech. That requires extra computational linguistics power such as a part of speech tagger. This allows it to do better resolutions (like resolving is and are to “be”).\n",
    "Another thing to note about lemmatization is that it’s often times harder to create a lemmatizer in a new language than it is a stemming algorithm. Because lemmatizers require a lot more knowledge about the structure of a language, it’s a much more intensive process than just trying to set up a heuristic stemming algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FZPHJp7Fl5_h"
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import wordnet\n",
    "\n",
    "#use for synonyms antonyms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 52
    },
    "id": "uazq028NX8Z_",
    "outputId": "ecd86004-b447-4e9f-9858-e94b91cc42b9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/wordnet.zip.\n"
     ]
    }
   ],
   "source": [
    "word=\"harsh\"\n",
    "nltk.download('wordnet')\n",
    "synonyms=wordnet.synsets(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 123
    },
    "id": "Nt_TqIW_YToo",
    "outputId": "58b37f62-f09f-41bb-ef66-98f5df2318b5"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Synset('harsh.s.01'),\n",
       " Synset('harsh.s.02'),\n",
       " Synset('coarse.a.01'),\n",
       " Synset('harsh.s.04'),\n",
       " Synset('harsh.s.05'),\n",
       " Synset('harsh.s.06')]"
      ]
     },
     "execution_count": 93,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "synonyms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Q0IZ-CNCYt44"
   },
   "outputs": [],
   "source": [
    "word2='power'\n",
    "sy=wordnet.synsets(word2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 194
    },
    "id": "B4QD-LmPZJJm",
    "outputId": "36e112c6-2d75-42b7-96bd-89430ffb74ef"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Synset('power.n.01'),\n",
       " Synset('power.n.02'),\n",
       " Synset('ability.n.02'),\n",
       " Synset('office.n.04'),\n",
       " Synset('power.n.05'),\n",
       " Synset('exponent.n.03'),\n",
       " Synset('might.n.01'),\n",
       " Synset('world_power.n.01'),\n",
       " Synset('baron.n.03'),\n",
       " Synset('power.v.01')]"
      ]
     },
     "execution_count": 95,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "id": "CWkJV3h5ZKdt",
    "outputId": "4e5fede9-92d9-4d8a-9aac-eb8a8add36a9"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'a mathematical notation indicating the number of times a quantity is multiplied by itself'"
      ]
     },
     "execution_count": 96,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a=sy[5]\n",
    "a.definition()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "id": "dMxtjIVUjQIj",
    "outputId": "4051a077-e456-4217-ea57-dd0951f8c74e"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'possession of the qualities (especially mental qualities) required to do something or get something done'"
      ]
     },
     "execution_count": 98,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a=sy[2]\n",
    "a.definition()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 123
    },
    "id": "q6V81H6IjfCK",
    "outputId": "cb344ac6-e48f-4f38-a0c4-8a8269a4377b"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Synset('win.n.01'),\n",
       " Synset('winnings.n.01'),\n",
       " Synset('win.v.01'),\n",
       " Synset('acquire.v.05'),\n",
       " Synset('gain.v.05'),\n",
       " Synset('succeed.v.01')]"
      ]
     },
     "execution_count": 97,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new=wordnet.synsets('win')\n",
    "new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4Ipzi68dvGPa"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "id": "6vin2AXOjrKt",
    "outputId": "88c455b9-08d6-467f-93b8-911d85216562"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'obtain advantages, such as points, etc.'"
      ]
     },
     "execution_count": 98,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b=new[4]\n",
    "b.definition()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "id": "_twHrwHGvIsV",
    "outputId": "2b25a6c7-4833-43c9-e2e7-e45b44a375bc"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'gain.v.05'"
      ]
     },
     "execution_count": 99,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b.name()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 52
    },
    "id": "UieVRrW1vJzf",
    "outputId": "5d5ab80d-63d7-4ec3-87d3-a1f721f49232"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['The home team was gaining ground',\n",
       " 'After defeating the Knicks, the Blazers pulled ahead of the Lakers in the battle for the number-one playoff berth in the Western Conference']"
      ]
     },
     "execution_count": 100,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b.examples()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "id": "6FHJYuWYqLDJ",
    "outputId": "d2fcb039-c8fe-48b2-955c-15180b1930b6"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'v'"
      ]
     },
     "execution_count": 101,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b.pos()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vZgJidVNs_lI"
   },
   "source": [
    "# ***What is Lemmatization?***\n",
    "Lemmatization is the algorithmic process of finding the lemma of a word depending on their meaning. Lemmatization usually refers to the morphological analysis of words, which aims to remove inflectional endings. It helps in returning the base or dictionary form of a word, which is known as the lemma. The NLTK Lemmatization method is based on WorldNet's built-in morph function. Text preprocessing includes both stemming as well as lemmatization. Many people find the two terms confusing. Some treat these as same, but there is a difference between these both. Lemmatization is preferred over the former because of the below reason."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5YybVWAFtfdF"
   },
   "source": [
    "# ***Why is Lemmatization better than Stemming?***\n",
    "Stemming algorithm works by cutting the suffix from the word. In a broader sense cuts either the beginning or end of the word.\n",
    "On the contrary, Lemmatization is a more powerful operation, and it takes into consideration morphological analysis of the words. It returns the lemma which is the base form of all its inflectional forms. In-depth linguistic knowledge is required to create dictionaries and look for the proper form of the word. Stemming is a general operation while lemmatization is an intelligent operation where the proper form will be looked in the dictionary. Hence, lemmatization helps in forming better machine learning features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VrsHrhUEqfmO"
   },
   "outputs": [],
   "source": [
    "ant=b.lemmas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "id": "4PuyNmGcq1Ap",
    "outputId": "94d52f6d-7ec9-4def-a830-e008d7f4b2eb"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Lemma('fall_back.v.04.fall_back')]"
      ]
     },
     "execution_count": 111,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b.lemmas()[0].antonyms()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AqSZv0hSrg45"
   },
   "outputs": [],
   "source": [
    "syn=wordnet.synsets('love')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 194
    },
    "id": "yxdU04gMrnkb",
    "outputId": "5c49772f-4619-4292-f32b-9122bf3f2aba"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Synset('love.n.01'),\n",
       " Synset('love.n.02'),\n",
       " Synset('beloved.n.01'),\n",
       " Synset('love.n.04'),\n",
       " Synset('love.n.05'),\n",
       " Synset('sexual_love.n.02'),\n",
       " Synset('love.v.01'),\n",
       " Synset('love.v.02'),\n",
       " Synset('love.v.03'),\n",
       " Synset('sleep_together.v.01')]"
      ]
     },
     "execution_count": 104,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "syn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "id7jJSpErnuo"
   },
   "outputs": [],
   "source": [
    "ax=[]\n",
    "bx=[]\n",
    "for i in syn:\n",
    "  for lem in i.lemmas():\n",
    "    ax.append(lem.name())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 834
    },
    "id": "56HiTg2Hrnx5",
    "outputId": "3dab2153-6949-469b-d6e6-87575909e0f7"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['love',\n",
       " 'love',\n",
       " 'passion',\n",
       " 'beloved',\n",
       " 'dear',\n",
       " 'dearest',\n",
       " 'honey',\n",
       " 'love',\n",
       " 'love',\n",
       " 'sexual_love',\n",
       " 'erotic_love',\n",
       " 'love',\n",
       " 'sexual_love',\n",
       " 'lovemaking',\n",
       " 'making_love',\n",
       " 'love',\n",
       " 'love_life',\n",
       " 'love',\n",
       " 'love',\n",
       " 'enjoy',\n",
       " 'love',\n",
       " 'sleep_together',\n",
       " 'roll_in_the_hay',\n",
       " 'love',\n",
       " 'make_out',\n",
       " 'make_love',\n",
       " 'sleep_with',\n",
       " 'get_laid',\n",
       " 'have_sex',\n",
       " 'know',\n",
       " 'do_it',\n",
       " 'be_intimate',\n",
       " 'have_intercourse',\n",
       " 'have_it_away',\n",
       " 'have_it_off',\n",
       " 'screw',\n",
       " 'fuck',\n",
       " 'jazz',\n",
       " 'eff',\n",
       " 'hump',\n",
       " 'lie_with',\n",
       " 'bed',\n",
       " 'have_a_go_at_it',\n",
       " 'bang',\n",
       " 'get_it_on',\n",
       " 'bonk']"
      ]
     },
     "execution_count": 106,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LYbSYDnbsTdr"
   },
   "outputs": [],
   "source": [
    "ax=[]\n",
    "bx=[]\n",
    "for i in syn:\n",
    "  for lem in i.lemmas():\n",
    "    for a in lem.antonyms():\n",
    "      ax.append(a.name())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "id": "l87pQo3Psvxx",
    "outputId": "21c9e07c-53aa-49b0-c2ac-2fb7a19a2408"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['hate', 'hate']"
      ]
     },
     "execution_count": 108,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ax"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AzHXoxGyxBmp"
   },
   "source": [
    "# ***Hypernyms and Hyponyms –***\n",
    "\n",
    "**Hypernyms:** More abstract terms\n",
    "\n",
    "**Hyponyms:** More specific terms.\n",
    "\n",
    "Both come to picture as Synsets are organized in a structure similar to that of an inheritance tree. This tree can be traced all the way up to a root hypernym. Hypernyms provide a way to categorize and group words based on their similarity to each other."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YKzr1-RzwLUO"
   },
   "outputs": [],
   "source": [
    "sy=wordnet.synsets('power')[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "id": "gcbs3VMGxrB9",
    "outputId": "b2b1dad2-558b-4ac7-adef-c4bd417167a5"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ability.n.02'"
      ]
     },
     "execution_count": 110,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sy.name()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "id": "Tnftqg2HxrFk",
    "outputId": "b30bb455-f1c7-469c-8fcc-d31de841e6ba"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['danger heightened his powers of discrimination']"
      ]
     },
     "execution_count": 111,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sy.examples()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "id": "1mRdWh0cwLXq",
    "outputId": "bcb9b708-c518-40a3-c942-ac89b78edb57"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'possession of the qualities (especially mental qualities) required to do something or get something done'"
      ]
     },
     "execution_count": 112,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sy.definition()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 247
    },
    "id": "v9OjyDyix1Hb",
    "outputId": "88b6b081-4231-47e0-eee3-b345c241324a"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Synset('aptitude.n.01'),\n",
       " Synset('bilingualism.n.01'),\n",
       " Synset('capacity.n.08'),\n",
       " Synset('creativity.n.01'),\n",
       " Synset('faculty.n.01'),\n",
       " Synset('hand.n.04'),\n",
       " Synset('intelligence.n.01'),\n",
       " Synset('know-how.n.01'),\n",
       " Synset('leadership.n.04'),\n",
       " Synset('originality.n.01'),\n",
       " Synset('skill.n.01'),\n",
       " Synset('skill.n.02'),\n",
       " Synset('superior_skill.n.01')]"
      ]
     },
     "execution_count": 113,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sy.hyponyms()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "id": "SazgevI9x1Kp",
    "outputId": "2024f8dc-bd85-48a8-e4f5-db1d8865a188"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Synset('cognition.n.01')]"
      ]
     },
     "execution_count": 114,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sy.hypernyms()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "id": "gxuf_wDcx1Of",
    "outputId": "cba447da-7f3d-43d0-ee73-95b3d02e707a"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'n'"
      ]
     },
     "execution_count": 115,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sy.pos()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aarG9D3Vnitx"
   },
   "source": [
    "# ***NLP | Synsets for a word in WordNet:***\n",
    "***WordNet:*** is the lexical database i.e. dictionary for the English language, specifically designed for natural language processing.\n",
    "\n",
    "***Synset*** is a special kind of a simple interface that is present in NLTK to look up words in WordNet. Synset instances are the groupings of synonymous words that express the same concept. Some of the words have only one Synset and some have several."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "m7b4YztxnhRo"
   },
   "outputs": [],
   "source": [
    "from nltk.stem import \tWordNetLemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vRXdhXhDj07C"
   },
   "outputs": [],
   "source": [
    "lemmmaa = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "j0Sn33lnuAY6"
   },
   "outputs": [],
   "source": [
    "text = \"studies studying cries cry\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "W99TqQBZuDEW"
   },
   "outputs": [],
   "source": [
    "tokenization = nltk.word_tokenize(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 87
    },
    "id": "W9bdCw1KuGUT",
    "outputId": "e32d2145-1e92-47a0-c8da-5e0c66cae774"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lemma for studies is study\n",
      "Lemma for studying is studying\n",
      "Lemma for cries is cry\n",
      "Lemma for cry is cry\n"
     ]
    }
   ],
   "source": [
    "for w in tokenization:\n",
    "\t\tprint(\"Lemma for {} is {}\".format(w, lemmmaa.lemmatize(w)))  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rp_wU7XovIRE"
   },
   "source": [
    "# ***How do we represent meaning of words?***\n",
    "If we use separate vectors for all 13 million words (or maybe more) in English vocabulary, we’ll be facing several problems. Firstly, we’ll have large vectors with a lot of ‘zeroes’ and one ‘one’ (in different position representing a different word). This is also known as one-hot encoding. Secondly, when we search for phrases such as “hotels in New Jersey” in Google, we want results pertaining to “motel”, “lodging”, “accommodation” in New Jersey returned as well. And if we are using one-hot encoding, these words have no natural notion of similarity. Ideally, we would want dot products (since we are dealing with vectors) of synonym / similar words to be close to one."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Gyxb3rgYv5nA"
   },
   "source": [
    "# ***Word2Vec***\n",
    "How can we build simple, scalable, fast to train models which can run over billions of words that will produce exceedingly good word representations? Let’s look into Word2Vec model to find answer to this.\n",
    "\n",
    "Word2Vec is a group of models which helps derive relations between a word and its contextual words. Let’s look at two important models inside Word2Vec: Skip-grams and CBOW\n",
    "\n",
    "In Skip-gram model, we take a centre word and a window of context (neighbor) words and we try to predict context words out to some window size for each centre word. So, our model is going to define a probability distribution i.e. probability of a word appearing in context given a centre word and we are going to choose our vector representations to maximize the probability."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "87CEcZDqwZEV"
   },
   "source": [
    "# ***Continuous Bag of Words model (CBOW)***\n",
    "In abstract terms, this is opposite of skip-gram. In CBOW, we try to predict centre word by summing vectors of surrounding words.\n",
    "This was about converting words into vectors. But where does the “learning” happen? Essentially, we begin with small random initialization of word vectors. Our predictive model learns the vectors by minimizing the loss function. In Word2vec, this happens with a feed-forward neural network and optimization techniques such as Stochastic gradient descent. There are also count-based models which make a co-occurrence count matrix of words in our corpus; we have a large matrix with each row for the “words” and columns for the “context”. The number of “contexts” is of course large, since it is essentially combinatorial in size. To overcome this size issue, we apply SVD to the matrix. This reduces the dimensions of the matrix retaining maximum information.\n",
    "\n",
    "# **Simple Way:**\n",
    "We need a way to represent text data for machine learning algorithm and the bag-of-words model helps us to achieve that task. The bag-of-words model is simple to understand and implement. It is a way of extracting features from the text for use in machine learning algorithms.\n",
    "\n",
    "In this approach, we use the tokenized words for each observation and find out the frequency of each token.\n",
    "Let’s take an example to understand this concept in depth.\n",
    "\n",
    "“It was the best of times”\n",
    "\n",
    "“It was the worst of times”\n",
    "\n",
    "“It was the age of wisdom”\n",
    "\n",
    "“It was the age of foolishness”\n",
    "\n",
    "\n",
    " ### We treat each sentence as a separate document and we make a list of all words from all the four documents excluding the punctuation. We get,\n",
    "\n",
    " ‘It’, ‘was’, ‘the’, ‘best’, ‘of’, ‘times’, ‘worst’, ‘age’, ‘wisdom’, ‘foolishness’\n",
    "\n",
    "### The next step is the create vectors. Vectors convert text that can be used by the machine learning algorithm.\n",
    "We take the first document — “It was the best of times” and we check the frequency of words from the 10 unique words.\n",
    "\n",
    "\n",
    "“it” = 1\n",
    "\n",
    "“was” = 1\n",
    "\n",
    "“the” = 1\n",
    "\n",
    "“best” = 1\n",
    "\n",
    "“of” = 1\n",
    "\n",
    "“times” = 1\n",
    "\n",
    "“worst” = 0\n",
    "\n",
    "“age” = 0\n",
    "\n",
    "“wisdom” = 0\n",
    "\n",
    "“foolishness” = 0\n",
    "\n",
    "### Rest of the documents will be:\n",
    "“It was the best of times” = [1, 1, 1, 1, 1, 1, 0, 0, 0, 0]\n",
    "\n",
    "“It was the worst of times” = [1, 1, 1, 0, 1, 1, 1, 0, 0, 0]\n",
    "\n",
    "“It was the age of wisdom” = [1, 1, 1, 0, 1, 0, 0, 1, 1, 0]\n",
    "\n",
    "“It was the age of foolishness” = [1, 1, 1, 0, 1, 0, 0, 1, 0, 1]\n",
    "\n",
    "# In this approach, each word or token is called a “gram”. Creating a vocabulary of two-word pairs is called a bigram model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_MZZ2DXjv4ru"
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "f-VSN6LpuM_e"
   },
   "outputs": [],
   "source": [
    "vect = CountVectorizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xEU7izwYz6v2"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yD1s4VPh0FEm"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qnRGSRZF0FRS"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "m5BohNzh0FOm"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8SSuYBKK0FMC"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "K1mgwYs10FJo"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fz2uEMAZ0GsS"
   },
   "source": [
    "# ***TF-IDF Vectorizer:***\n",
    "\n",
    "TF-IDF stands for term frequency-inverse document frequency. TF-IDF weight is a statistical measure used to evaluate how important a word is to a document in a collection or corpus. The importance increases proportionally to the number of times a word appears in the document but is offset by the frequency of the word in the corpus.\n",
    "\n",
    "\n",
    "*  ***Term Frequency (TF):*** is a scoring of the frequency of the word in the current document. Since every document is different in length, it is possible that a term would appear much more times in long documents than shorter ones. The term frequency is often divided by the document length to normalize.\n",
    "\n",
    "![alt text](https://miro.medium.com/max/808/1*SUAeubfQGK_w0XZWQW6V1Q.png)\n",
    "\n",
    "*   ***Inverse Document Frequency (IDF):*** is a scoring of how rare the word is across documents. IDF is a measure of how rare a term is. Rarer the term, more is the IDF score.\n",
    "\n",
    "\n",
    "![alt text](https://miro.medium.com/max/822/1*T57j-UDzXizqG40FUfmkLw.png)\n",
    "\n",
    "Thus\n",
    "\n",
    "![alt text](https://miro.medium.com/max/215/1*YrgmAeG7KNRB4dQcGcsdyg.png)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_8R9qQ1-0FHn"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "NLP_Basic.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
